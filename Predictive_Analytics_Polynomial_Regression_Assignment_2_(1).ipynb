{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Description:\n",
        "A Human Resource company needs to determine the salary for a new job position being created. They only have access to a salary dataset for the company, which includes salary information for the top 10 positions along with their corresponding levels. The task is to assist HR in deciding the appropriate salary if the new position falls between levels 7 and 8.\n",
        "\n",
        "Data Set: **Position_Salaries.csv**\n",
        "Rules: You are restricted from utilizing the sklearn library.You are limited to using only the libraries provided.\n",
        "\n",
        "Reference : https://www.javatpoint.com/machine-learning-polynomial-regression\n",
        "\n",
        " https://www.kaggle.com/code/omkarsantoshraut/polynomial-regression"
      ],
      "metadata": {
        "id": "AtzSquaQnaBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowded to use only these libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "1tJn4PzWnZtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory data analysis"
      ],
      "metadata": {
        "id": "pDonG1iOtiCP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUFCZbfRlfzi"
      },
      "outputs": [],
      "source": [
        "# your code(s)\n",
        "\n",
        "data=pd.read_csv(r\"D:\\DUK\\sem 2\\predictive\\note\\Position_Salaries.csv\")\n",
        "data.head()\n",
        "print(data.describe())\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Level', y='Salary', data=data)\n",
        "plt.title('Salary vs Level')\n",
        "plt.xlabel('Level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on the exploratory data analysis, answer the following\n",
        "\n",
        "\n",
        "* Can you use a simple linear regression model to fit this data (Yes/No)?\n",
        "> * NO\n",
        "* If you use a simple linear model to fit this data, will it fit well (Yes/No)?\n",
        "> * NO\n",
        "* Explain why you said yes or no to the above questions.\n",
        "> * We cannot use a linear model in this case because the data does not exhibit a linear relationship. Using a linear model would result in underfitting and fail to produce the desired outcome.\n",
        "\n"
      ],
      "metadata": {
        "id": "mvaBWQfAtpp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomial Regression\n",
        "\n",
        "**Construct a second order Model:** $$\\;\\;\\;\\;\n",
        "\\hat y = w_1 x_1^2 + w_2x_1 + b\n",
        "$$\n",
        "\n",
        "**Construct a Cost function:**\n",
        "$$MSE(w_1,w_2,b)= your\\;answer$$"
      ],
      "metadata": {
        "id": "exOvj2CxFelv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code to plot the cost function (if required)\n",
        "X = data['Level'].values.reshape(-1, 1)\n",
        "y = data['Salary'].values\n",
        "X_poly = np.hstack((X, X**2))\n",
        "X_poly = np.hstack((np.ones((X_poly.shape[0], 1)), X_poly))\n",
        "coefficients = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
        "\n",
        "def poly_model(x):\n",
        "    return coefficients[0] + coefficients[1] * x + coefficients[2] * x**2\n",
        "\n",
        "level_new = 7.5\n",
        "predicted_salary = poly_model(level_new)\n",
        "print(f\"Predicted Salary for level {level_new}: ${predicted_salary:.2f}\")\n",
        "\n",
        "# Plotting the polynomial curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='red')\n",
        "plt.plot(np.linspace(min(X), max(X), 100), poly_model(np.linspace(min(X), max(X), 100)), color='blue')\n",
        "plt.title('Polynomial Regression: Salary vs Level')\n",
        "plt.xlabel('Level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Da1vxqoEyL-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By analysing the cost function, answer the following\n",
        "\n",
        "\n",
        "* Can you use mean squared error as the cost function (Yes/No)?\n",
        "> * Yes\n",
        "* Explain why you said yes or no to the above questions.\n",
        "> * The cost function is  for polynomial regression because it calculates the average squared difference between the predicted values and the actual values. This makes it effective for assessing the performance of the polynomial regression model."
      ],
      "metadata": {
        "id": "2aQT57RJySAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Derivatives of cost function:**\n",
        "Derivative with respect to \\(w_1\\) (weight of \\(x^2\\)):\n",
        "  $$\n",
        "  \\frac{\\partial MSE}{\\partial w_1} = -\\frac{2}{n} \\sum_{i=1}^n \\left(y_i - (w_1 x_{1i}^2 + w_2 x_{1i} + b)\\right) x_{1i}^2\n",
        "  $$\n",
        "\n",
        "- Derivative with respect to \\(w_2\\) (weight of \\(x\\)):\n",
        "  $$\n",
        "  \\frac{\\partial MSE}{\\partial w_2} = -\\frac{2}{n} \\sum_{i=1}^n \\left(y_i - (w_1 x_{1i}^2 + w_2 x_{1i} + b)\\right) x_{1i}\n",
        "  $$\n",
        "\n",
        "- Derivative with respect to \\(b\\) (bias):\n",
        "  $$\n",
        "  \\frac{\\partial MSE}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^n \\left(y_i - (w_1 x_{1i}^2 + w_2 x_{1i} + b)\\right)\n",
        "  $$\n",
        "\n",
        "\n",
        "**Gradient Descent Algorithm:**\n",
        "```\n",
        "Repeat until converges:\n",
        "```\n",
        "$$ Δw_1 = -\\frac{2}{n} \\sum_{i=1}^n \\left( y_i - (w_1 x_{1i}^2 + w_2 x_{1i} + b) \\right) x_{1i}^2 \\\n",
        "       $$ $$Δw_2 = -\\frac{2}{n} \\sum_{i=1}^n \\left( y_i - (w_1 x_{1i}^2 + w_2 x_{1i} + b) \\right) x_{1i}$$\n",
        "       $$ Δb = -\\frac{2}{n} \\sum_{i=1}^n \\left( y_i - (w_1 x_{1i}^2 + w_2 x_{1i} + b) \\right)$$"
      ],
      "metadata": {
        "id": "sTQ46QsyyuYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of the model"
      ],
      "metadata": {
        "id": "7bzYFCh5zIL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Polynomial Regression Model Function\n",
        "def polynomial_regression(x, coefficients):\n",
        "    \"\"\" Calculates y_pred = b + w1*x + w2*x^2 \"\"\"\n",
        "    y_pred = np.dot(x, coefficients)\n",
        "    return y_pred\n",
        "\n",
        "# Cost Function - Mean Squared Error\n",
        "def compute_cost(X, y, coefficients):\n",
        "    \"\"\" Computes the mean squared error cost \"\"\"\n",
        "    n = len(y)\n",
        "    y_pred = polynomial_regression(X, coefficients)\n",
        "    cost = (1/(2*n)) * np.sum((y_pred - y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# Derivative of Cost Function\n",
        "def derivatives(X, y, coefficients):\n",
        "    \"\"\" Computes the derivatives of the MSE cost function \"\"\"\n",
        "    n = len(y)\n",
        "    y_pred = polynomial_regression(X, coefficients)\n",
        "    d_coefficients = (1/n) * np.dot(X.T, (y_pred - y))\n",
        "    return d_coefficients\n",
        "\n",
        "# Gradient Descent Function to Minimize the Cost Function\n",
        "def gradient_descent(X, y, coefficients, learning_rate, iterations):\n",
        "    \"\"\" Performs gradient descent to learn coefficients \"\"\"\n",
        "    cost_history = np.zeros(iterations)\n",
        "    for i in range(iterations):\n",
        "        coefficients -= learning_rate * derivatives(X, y, coefficients)\n",
        "        cost_history[i] = compute_cost(X, y, coefficients)\n",
        "    return coefficients, cost_history\n",
        "\n",
        "# Load the dataset\n",
        "data=pd.read_csv(r\"D:\\DUK\\sem 2\\predictive\\note\\Position_Salaries.csv\")\n",
        "X = data['Level'].values.reshape(-1, 1)\n",
        "y = data['Salary'].values\n",
        "\n",
        "# Prepare polynomial features: x, x^2\n",
        "X_poly = np.hstack((np.ones((X.shape[0], 1)), X, X**2))\n",
        "# Initial coefficients (b, w1, w2)\n",
        "coefficients = np.zeros(3)\n",
        "learning_rate = 0.0001\n",
        "iterations = 10000\n",
        "\n",
        "# Run gradient descent\n",
        "coefficients, cost_history = gradient_descent(X_poly, y, coefficients, learning_rate, iterations)\n",
        "\n",
        "# Predicting using optimized coefficients\n",
        "y_pred = polynomial_regression(X_poly, coefficients)\n",
        "# Plot polynomial model\n",
        "plt.scatter(X, y, color='red', label='Actual data')\n",
        "plt.plot(X, y_pred, label='Polynomial regression fit')\n",
        "plt.xlabel('Level')\n",
        "plt.ylabel('Salary')\n",
        "plt.title('Polynomial Regression Fit')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot iteration vs cost\n",
        "plt.plot(range(iterations), cost_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost reduction over iterations')\n",
        "plt.show()\n",
        "# New level prediction\n",
        "level_new = 7.5\n",
        "new_X = np.array([1, level_new, level_new**2]).reshape(1, -1)\n",
        "predicted_salary = polynomial_regression(new_X, coefficients)\n",
        "\n",
        "# Plot with the new prediction\n",
        "plt.scatter(X, y, color='red')\n",
        "plt.plot(X, y_pred, label='Polynomial regression fit')\n",
        "plt.scatter([level_new], [predicted_salary], color='green', label=f'Predicted salary for level {level_new}')\n",
        "plt.xlabel('Level')\n",
        "plt.ylabel('Salary')\n",
        "plt.title('Polynomial Regression Fit with Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Predicted Salary for level {level_new}: ${predicted_salary[0]:,.2f}\")"
      ],
      "metadata": {
        "id": "EPZv6kMJzfk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer the following\n",
        "\n",
        "* What is learning rate?\n",
        "> * It is the parameter that determines the step size taken during the optimization process in gradient descent.\n",
        "* What will happen if the learning rate is too large?\n",
        "> * If it is too large the the step size will be large and it may not converge to minima.it will overshoot the minimum, leading to divergence or oscillations around the optimal values\n",
        "* What will happen if the learning rate is too small?\n",
        "> * It will converge but the traning time will be too long(slow traning time)\n",
        "* If you what to change the second order (quadratic) model to third order model what all things will change in the above code?\n",
        "> * Add a term for the cubic power of x, changing from:\n",
        "$$ y = w_1 x^2 + w_2 x + b $$\n",
        "to:\n",
        "$$ y = w_3 x^3 + w_1 x^2 + w_2 x + b $$\n",
        "\n",
        " Gradient Calculation for w_3:Introduce a new gradient calculation for w_3:$$\n",
        " \\frac{\\partial MSE}{\\partial w_3} = -\\frac{2}{n} \\sum_{i=1}^{n} \\left(y_i - (w_3 x_i^3 + w_1 x_i^2 + w_2 x_i + b)\\right) x_i^3 $$\n",
        "\n",
        " Update Rule for w_3:\n",
        "Update w_3 in each iteration:\n",
        " w_3 :$$w_3= w_3 - \\alpha \\frac{\\partial MSE}{\\partial w_3} $$\n",
        "\n",
        " Initialization:\n",
        "Start with an initial value for w_3 similar to other parameters.\n"
      ],
      "metadata": {
        "id": "qhvdbzMx0P-B"
      }
    }
  ]
}